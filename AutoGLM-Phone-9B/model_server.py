#!/usr/bin/env python3
"""
AutoGLM-Phone-9B æ¨¡å‹æœåŠ¡
æä¾› OpenAI å…¼å®¹çš„ API æ¥å£
"""

import os
import sys
import json
import torch
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ  Open-AutoGLM è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "Open-AutoGLM"))

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
    from flask import Flask, request, jsonify
    import yaml
except ImportError:
    print("æ­£åœ¨å®‰è£…ä¾èµ–...")
    os.system("pip install transformers flask torch pyyaml")
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
    from flask import Flask, request, jsonify
    import yaml

app = Flask(__name__)

class ModelServer:
    def __init__(self, model_path: str, host: str = "localhost", port: int = 8000):
        self.model_path = model_path
        self.host = host
        self.port = port
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"æ¨¡å‹æœåŠ¡é…ç½®:")
        print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"  æœåŠ¡åœ°å€: {host}:{port}")
        print(f"  è®¾å¤‡: {self.device}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # ä½¿ç”¨ AutoModel è€Œä¸æ˜¯ AutoModelForCausalLMï¼Œå› ä¸º AutoGLM-Phone-9B ä½¿ç”¨ç‰¹æ®Šçš„é…ç½®ç±»
            self.model = AutoModel.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
            
        except Exception as e:
            print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate_response(self, messages: List[Dict[str, str]], 
                         max_tokens: int = 3000, 
                         temperature: float = 0.1) -> str:
        """ç”Ÿæˆå“åº”"""
        try:
            # æ„å»ºè¾“å…¥æ–‡æœ¬
            if isinstance(messages, str):
                input_text = messages
            else:
                # ç®€åŒ–çš„æ¶ˆæ¯æ ¼å¼å¤„ç†
                input_text = ""
                for msg in messages:
                    if msg.get("role") == "user":
                        input_text = msg.get("content", "")
                        break
            
            if not input_text:
                return "è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜"
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
            if input_text in response:
                response = response.replace(input_text, "").strip()
            
            return response
            
        except Exception as e:
            print(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
            return f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"

# åˆ›å»ºå…¨å±€æ¨¡å‹æœåŠ¡å®ä¾‹
model_server = None

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    try:
        data = request.get_json()
        
        messages = data.get('messages', [])
        max_tokens = data.get('max_tokens', 3000)
        temperature = data.get('temperature', 0.1)
        model = data.get('model', 'autoglm-phone-9b')
        
        # ç”Ÿæˆå“åº”
        response_text = model_server.generate_response(
            messages, max_tokens, temperature
        )
        
        # æ„å»º OpenAI å…¼å®¹çš„å“åº”æ ¼å¼
        response = {
            "id": f"chatcmpl-{hash(str(messages))}",
            "object": "chat.completion",
            "created": int(torch.cuda.Event().elapsed_time(torch.cuda.Event()) / 1000) if torch.cuda.is_available() else 0,
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(str(messages)),
                "completion_tokens": len(response_text),
                "total_tokens": len(str(messages)) + len(response_text)
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            "error": {
                "message": str(e),
                "type": "internal_error",
                "code": "internal_error"
            }
        }), 500

@app.route('/v1/models', methods=['GET'])
def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return jsonify({
        "object": "list",
        "data": [{
            "id": "autoglm-phone-9b",
            "object": "model",
            "created": 0,
            "owned_by": "autoglm"
        }]
    })

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        "status": "healthy",
        "model_loaded": model_server.model is not None,
        "device": model_server.device
    })

def main():
    global model_server
    
    # é»˜è®¤æ¨¡å‹è·¯å¾„
    model_path = "./autoglm-phone-9b"
    host = "localhost"
    port = 8000
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    if len(sys.argv) > 2:
        host = sys.argv[2]
    if len(sys.argv) > 3:
        port = int(sys.argv[3])
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not Path(model_path).exists():
        print(f"âœ— æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹: python download_autoglm_model.py")
        return False
    
    # åˆ›å»ºå¹¶å¯åŠ¨æ¨¡å‹æœåŠ¡
    model_server = ModelServer(model_path, host, port)
    
    if not model_server.load_model():
        return False
    
    print(f"\nğŸš€ æ¨¡å‹æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    print(f"æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"API ç«¯ç‚¹:")
    print(f"  - POST /v1/chat/completions")
    print(f"  - GET  /v1/models")
    print(f"  - GET  /health")
    print(f"\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    try:
        app.run(host=host, port=port, debug=False)
    except KeyboardInterrupt:
        print("\næ­£åœ¨åœæ­¢æœåŠ¡...")
        print("æœåŠ¡å·²åœæ­¢")

if __name__ == "__main__":
    main()
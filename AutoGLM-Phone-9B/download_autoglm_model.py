#!/usr/bin/env python3
"""
AutoGLM-Phone-9B æ¨¡å‹ä¸‹è½½è„šæœ¬
æ”¯æŒä» Hugging Face å’Œ ModelScope ä¸‹è½½æ¨¡å‹
"""

import os
import sys
import subprocess
from pathlib import Path

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("æ­£åœ¨å®‰è£…ä¾èµ–...")
    dependencies = [
        "huggingface_hub",
        "modelscope"
    ]
    
    for dep in dependencies:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", dep])
            print(f"âœ“ å·²å®‰è£… {dep}")
        except subprocess.CalledProcessError:
            print(f"âœ— å®‰è£… {dep} å¤±è´¥")
            return False
    return True

def download_from_huggingface():
    """ä» Hugging Face ä¸‹è½½æ¨¡å‹"""
    print("\n=== ä» Hugging Face ä¸‹è½½ AutoGLM-Phone-9B ===")
    
    try:
        from huggingface_hub import snapshot_download
        
        model_id = "zai-org/AutoGLM-Phone-9B"
        local_dir = "./autoglm-phone-9b"
        
        print(f"æ¨¡å‹ID: {model_id}")
        print(f"ä¸‹è½½ç›®å½•: {local_dir}")
        
        # åˆ›å»ºä¸‹è½½ç›®å½•
        Path(local_dir).mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½æ¨¡å‹
        snapshot_download(
            repo_id=model_id,
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            resume_download=True
        )
        
        print("âœ“ Hugging Face ä¸‹è½½å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âœ— Hugging Face ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_from_modelscope():
    """ä» ModelScope ä¸‹è½½æ¨¡å‹"""
    print("\n=== ä» ModelScope ä¸‹è½½ AutoGLM-Phone-9B ===")
    
    try:
        from modelscope import snapshot_download
        
        model_id = "ZhipuAI/AutoGLM-Phone-9B"
        local_dir = "./autoglm-phone-9b"
        
        print(f"æ¨¡å‹ID: {model_id}")
        print(f"ä¸‹è½½ç›®å½•: {local_dir}")
        
        # ä¸‹è½½æ¨¡å‹
        snapshot_download(
            model_id=model_id,
            local_dir=local_dir,
            cache_dir="./cache"
        )
        
        print("âœ“ ModelScope ä¸‹è½½å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âœ— ModelScope ä¸‹è½½å¤±è´¥: {e}")
        return False

def verify_model_files():
    """éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    print("\n=== éªŒè¯æ¨¡å‹æ–‡ä»¶ ===")
    
    model_dir = Path("./autoglm-phone-9b")
    
    if not model_dir.exists():
        print("âœ— æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    required_files = [
        "config.json",
        "pytorch_model.bin",
        "tokenizer.json"
    ]
    
    missing_files = []
    for file in required_files:
        file_path = model_dir / file
        if not file_path.exists():
            missing_files.append(file)
        else:
            size = file_path.stat().st_size / (1024 * 1024)  # MB
            print(f"âœ“ {file}: {size:.1f} MB")
    
    if missing_files:
        print(f"âœ— ç¼ºå¤±æ–‡ä»¶: {missing_files}")
        return False
    
    print("âœ“ æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡ï¼")
    return True

def create_model_server_config():
    """åˆ›å»ºæ¨¡å‹æœåŠ¡é…ç½®æ–‡ä»¶"""
    print("\n=== åˆ›å»ºæ¨¡å‹æœåŠ¡é…ç½® ===")
    
    config_content = """
# AutoGLM-Phone-9B æ¨¡å‹æœåŠ¡é…ç½®
model_name: "autoglm-phone-9b"
model_path: "./autoglm-phone-9b"
host: "localhost"
port: 8000
max_tokens: 3000
temperature: 0.1

# æ¨ç†é…ç½®
batch_size: 1
dtype: "float16"
device: "cuda"  # æˆ– "cpu"

# æ—¥å¿—é…ç½®
log_level: "INFO"
"""
    
    config_path = "model_server_config.yaml"
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"âœ“ é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return config_path

def main():
    """ä¸»å‡½æ•°"""
    print("AutoGLM-Phone-9B æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 40)
    
    # å®‰è£…ä¾èµ–
    if not install_dependencies():
        print("ä¾èµ–å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…åé‡è¯•")
        return False
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ¨¡å‹
    if Path("./autoglm-phone-9b").exists():
        print("å‘ç°å·²æœ‰æ¨¡å‹ç›®å½•ï¼Œè¿›è¡ŒéªŒè¯...")
        if verify_model_files():
            print("æ¨¡å‹å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œè·³è¿‡ä¸‹è½½")
            create_model_server_config()
            return True
        else:
            print("æ¨¡å‹ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...")
    
    # å°è¯•ä» Hugging Face ä¸‹è½½
    success = download_from_huggingface()
    
    # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä» ModelScope ä¸‹è½½
    if not success:
        print("Hugging Face ä¸‹è½½å¤±è´¥ï¼Œå°è¯• ModelScope...")
        success = download_from_modelscope()
    
    if success:
        # éªŒè¯ä¸‹è½½çš„æ¨¡å‹
        if verify_model_files():
            create_model_server_config()
            print("\nğŸ‰ æ¨¡å‹ä¸‹è½½å’Œé…ç½®å®Œæˆï¼")
            print("\nä¸‹ä¸€æ­¥:")
            print("1. å¯åŠ¨æ¨¡å‹æœåŠ¡: python model_server.py")
            print("2. è¿è¡Œå·¥ä½œè„šæœ¬: python autoglm_workscript.py <device_id>")
            return True
        else:
            print("\nâœ— æ¨¡å‹éªŒè¯å¤±è´¥")
            return False
    else:
        print("\nâœ— æ‰€æœ‰ä¸‹è½½æ–¹å¼éƒ½å¤±è´¥äº†")
        print("\nå»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ./autoglm-phone-9b ç›®å½•")
        print("3. ä½¿ç”¨ä»£ç†æˆ–VPNè®¿é—® Hugging Face")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)